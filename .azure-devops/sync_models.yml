# Copyright (c) Microsoft Corporation MIT license
#
# Azure IoT Models Repository synchronization pipeline definition
# The following pipeline variables are required:
#
# DMR_SERVICE_CONNECTION
# DMR_STORAGE_ACCOUNT

variables:
 - name: dmrClientVer
   value: 1.0.0-beta.3
 - name: indexStagingPath
   value: ./index_pages/
 - name: pageLimit
   value: 2048

# Required for schedule trigger
trigger: none
pr: none

name: 
schedules:
- cron: '0 */1 * * *'
  displayName: 'Scheduled models sync event'
  branches:
    include: 
    - 'main'
  always: false

jobs:
- job: 'sync_models'
  displayName: 'Sync models'
  pool:
    vmImage: 'ubuntu-20.04'
  steps:
  - task: AzureCLI@2
    displayName: 'Synchronizing'
    inputs:
      azureSubscription: $(DMR_SERVICE_CONNECTION)
      scriptType: bash
      scriptLocation: inlineScript
      inlineScript: |
        set -e

        echo "Synchronizing models..."
        az storage blob upload-batch -s "./dtmi" -d '$web/dtmi' --account-name "$DMR_STORAGE_ACCOUNT" --if-unmodified-since 2018-01-01T01:01:01Z --auth-mode login --pattern "*.json"
    env:
      DMR_STORAGE_ACCOUNT: $(DMR_STORAGE_ACCOUNT)

- job: 'evaluate_repo'
  displayName: 'Evaluate repository'
  pool:
    vmImage: 'ubuntu-20.04'
  steps:
  - task: Bash@3
    displayName: 'Expand and index repository models'
    inputs:
      targetType: 'inline'
      script: |
        set -e
        
        git clone -b $(dmrClientVer) https://github.com/Azure/iot-plugandplay-models-tools ../dmrtools
        PATH=../dmrtools:$PATH
        dotnet pack ../dmrtools/clients/dotnet -v m -c Release
        dotnet tool install Microsoft.IoT.ModelsRepository.CommandLine --tool-path ../dmrtools --add-source ../dmrtools/clients/dotnet/Microsoft.IoT.ModelsRepository.CommandLine/bin/Release/ --version $(dmrClientVer)

        echo "Expanding repository..."
        dmr-client expand --local-repo $PWD

        echo "Generating index..."
        dmr-client index --local-repo $PWD -o $(indexStagingPath)/index.json --page-limit $(pageLimit)

  - task: PythonScript@0
    displayName: 'Generate snapshot metadata'
    inputs:
      scriptSource: 'inline'
      script: |
        import os
        import json
        from datetime import datetime, timezone

        def scantree(path):
          for entry in os.scandir(path):
            if entry.is_dir(follow_symlinks=False):
              yield from scantree(entry.path)
            else:
              yield entry
        
        model_count = 0
        index_path = os.environ["INDEX_PATH"]
        for index_page in scantree(index_path):
          with open(index_page, 'r', encoding='utf-8') as f:
            index_page_dict = json.loads(f.read())
            model_count = model_count + len(index_page_dict.get("models", {}))

        metadata = {
          "totalModelCount": model_count,
          "publishDateUtc": datetime.now(timezone.utc).isoformat(),
          "commitId": os.environ.get("COMMIT_ID"),
          "sourceRepo": os.environ.get("REPO_NAME"),
          "features": {
            "index": True,
            "expanded": True
          }
        }
        with open(os.path.join(index_path, "metadata.json"), 'x') as f:
          f.write(json.dumps(metadata, indent=2, sort_keys=True))
    env:
      INDEX_PATH: $(indexStagingPath)
      COMMIT_ID: $(Build.SourceVersion)
      REPO_NAME: $(Build.Repository.Name)

  - task: PublishBuildArtifacts@1
    displayName: 'Publishing expanded models container'
    inputs:
      pathToPublish: './dtmi'
      publishLocation: 'Container' 
      artifactName: 'expanded_models'
      StoreAsTar: true

  - task: PublishBuildArtifacts@1
    displayName: 'Publishing metadata container'
    inputs:
      pathToPublish: $(indexStagingPath)
      publishLocation: 'Container' 
      artifactName: 'metadata'
      StoreAsTar: true

- job: 'sync_expanded_models'
  displayName: 'Sync expanded models'
  dependsOn: 'evaluate_repo'
  pool:
    vmImage: 'ubuntu-20.04'
  steps:
  - checkout: none
  - task: DownloadBuildArtifacts@0
    displayName : 'Downloading expanded models container'
    inputs:
      buildType: 'current'
      downloadType: 'single'
      artifactName: 'expanded_models'
      downloadPath: '$(System.ArtifactsDirectory)'

  - task: AzureCLI@2
    displayName: 'Synchronizing'
    inputs:
      azureSubscription: $(DMR_SERVICE_CONNECTION)
      scriptType: bash
      scriptLocation: inlineScript
      inlineScript: |
        set -e
        echo "Extracting expanded models content..."
        mkdir "$EXPANDED_MODELS_PATH/dtmi" && tar -xf "$EXPANDED_MODELS_PATH/expanded_models.tar" -C "$EXPANDED_MODELS_PATH/dtmi"

        echo "Synchronizing expanded models..."
        az storage blob upload-batch -s "$EXPANDED_MODELS_PATH/dtmi" -d '$web/dtmi' --account-name "$DMR_STORAGE_ACCOUNT" --if-unmodified-since 2018-01-01T01:01:01Z --auth-mode login --pattern "*.expanded.json"
    env:
      DMR_STORAGE_ACCOUNT: $(DMR_STORAGE_ACCOUNT)
      EXPANDED_MODELS_PATH: '$(System.ArtifactsDirectory)/expanded_models/'

- job: 'sync_metadata'
  displayName: 'Sync  metadata'
  dependsOn: ['evaluate_repo', 'sync_models']
  pool:
    vmImage: 'ubuntu-20.04'
  steps:
  - checkout: none
  - task: DownloadBuildArtifacts@0
    displayName : 'Downloading metadata container'
    inputs:
      buildType: 'current'
      downloadType: 'single'
      artifactName: 'metadata'
      downloadPath: '$(System.ArtifactsDirectory)'

  - task: AzureCLI@2
    displayName: 'Synchronizing'
    inputs:
      azureSubscription: $(DMR_SERVICE_CONNECTION)
      scriptType: bash
      scriptLocation: inlineScript
      inlineScript: |
        set -e
        echo "Extracting metadata content..."
        tar -xf "$METADATA_PATH/metadata.tar" -C "$METADATA_PATH"

        echo "Synchronizing index and metadata..."
        az storage blob upload-batch -s "$METADATA_PATH" -d '$web' --account-name "$DMR_STORAGE_ACCOUNT" --auth-mode login --pattern "*.json"
    env:
      DMR_STORAGE_ACCOUNT: $(DMR_STORAGE_ACCOUNT)
      METADATA_PATH: '$(System.ArtifactsDirectory)/metadata/'
